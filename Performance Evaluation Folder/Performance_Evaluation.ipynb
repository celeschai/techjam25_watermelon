{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da6a110",
   "metadata": {},
   "source": [
    "# Performance Evaluation\n",
    "\n",
    "We compare our classification results (pipeline_output.csv) against the ground_truth file (sample_gt.csv), whose labels were manually annotated by us. We assume this file to represent the correct ground truth.\n",
    "The pipeline_output.csv is generated based on past executions of the 'sample_test.csv' using the gemma_pipeline.py model. If you wish to re-run the model and use the new corresponding pipeline_output.csv, do the following:\n",
    "\n",
    "1. run the 'sample_test.csv' under the 'Performance Evaluation Folder' inside the gemma_pipeline.py model\n",
    "2. drag the 'pipeline_output.csv' into the 'Performance Evaluation Folder'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85c4f33",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0504903a-0573-422d-99e0-19dd4c7231f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe79cd0-783f-4843-bb49-954a9b621bd5",
   "metadata": {},
   "source": [
    "## Load files\n",
    "- load the test data set ('pipeline_output.csv')\n",
    "- load the ground_truth data set ('sample_gt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b6a505-f469-4df3-98ce-411ab24b3bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = pd.read_csv('pipeline_output.csv')\n",
    "ground_truth = pd.read_csv('sample_gt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beecd026-fc4b-4c95-8cb0-d4154a4d46c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_cols = [\"is_text_ad\", \"is_image_ad\", \"is_image_irrelevant\", \"is_text_irrelevant\", \"is_text_rant\", \"is_review_ad\", \"is_review_irrelevant\", \"sensibility\"]\n",
    "for col in bool_cols:\n",
    "    test_file[col] = test_file[col].astype(bool)\n",
    "test_file[\"is_review_ad\"] = test_file[\"is_text_ad\"] | test_file[\"is_image_ad\"]\n",
    "test_file['is_review_irrelevant'] = test_file[\"is_image_irrelevant\"] | test_file[\"is_text_irrelevant\"]\n",
    "# print(test_file.columns)\n",
    "# test_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f6750e-559c-492c-9110-e11d05a22575",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_cols_gt = [\"is_text_rant\", \"is_review_ad\", \"is_review_irrelevant\", \"sensibility\"]\n",
    "for col in bool_cols_gt:\n",
    "    ground_truth[col] = ground_truth[col].astype(bool)\n",
    "# print(ground_truth.columns)\n",
    "# ground_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f09808-377a-4a91-a471-439030e3239c",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "We evalute precision, recall, and F1-score for each class (True/False) and the overall weighted/macro scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee140a7-754c-4d90-9fec-4de24c2b2026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== is_review_irrelevant ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.961     0.813     0.881        91\n",
      "        True      0.261     0.667     0.375         9\n",
      "\n",
      "    accuracy                          0.800       100\n",
      "   macro avg      0.611     0.740     0.628       100\n",
      "weighted avg      0.898     0.800     0.835       100\n",
      "\n",
      "=== is_review_ad ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      1.000     0.990     0.995       100\n",
      "        True      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.990       100\n",
      "   macro avg      0.500     0.495     0.497       100\n",
      "weighted avg      1.000     0.990     0.995       100\n",
      "\n",
      "=== is_text_rant ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      0.984     0.733     0.840        86\n",
      "        True      0.361     0.929     0.520        14\n",
      "\n",
      "    accuracy                          0.760       100\n",
      "   macro avg      0.673     0.831     0.680       100\n",
      "weighted avg      0.897     0.760     0.795       100\n",
      "\n",
      "=== sensibility ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      1.000     0.182     0.308        11\n",
      "        True      0.908     1.000     0.952        89\n",
      "\n",
      "    accuracy                          0.910       100\n",
      "   macro avg      0.954     0.591     0.630       100\n",
      "weighted avg      0.918     0.910     0.881       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets = [\"is_review_irrelevant\", \"is_review_ad\", \"is_text_rant\", \"sensibility\"]\n",
    "\n",
    "pred_cols = [\"review_id\"] + [f\"{col}\" for col in targets] + [\"helpfulness\"]\n",
    "gt_cols = [\"review_id\"] + [f\"{col}\" for col in targets] + [\"helpfulness\"]\n",
    "\n",
    "test_subset = test_file[pred_cols]\n",
    "gt_subset = ground_truth[gt_cols]\n",
    "\n",
    "df = test_subset.merge(gt_subset, on=\"review_id\", suffixes=(\"_pred\", \"_true\"))\n",
    "\n",
    "for col in targets:\n",
    "    y_true = df[f\"{col}_true\"]\n",
    "    y_pred = df[f\"{col}_pred\"]\n",
    "    print(f\"=== {col} ===\")\n",
    "    print(classification_report(y_true, y_pred, digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffca02d-f101-4baf-a3e6-6dfc3eba9278",
   "metadata": {},
   "source": [
    "We subsequently iterated the process with alternative prompts. The prompts are listed in the main report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
