{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7322cc6-e069-4fb4-a2da-44c2ef1ea840",
   "metadata": {},
   "source": [
    "We compare our classification results against the ground_truth file, whose labels were manually annotated by us. We assume this file to represent the correct ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0504903a-0573-422d-99e0-19dd4c7231f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe79cd0-783f-4843-bb49-954a9b621bd5",
   "metadata": {},
   "source": [
    "We load the model test_file and the ground_truth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3b6a505-f469-4df3-98ce-411ab24b3bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = pd.read_csv('vt_merged.csv').head(20)\n",
    "test_file[\"is_review_ad\"] = test_file[\"is_text_ad\"] | test_file[\"is_image_ad\"]\n",
    "test_file['is_review_irrelevant'] = test_file[\"is_image_irrelevant\"] | test_file[\"is_text_irrelevant\"]\n",
    "ground_truth = pd.read_csv('vt_merged.csv').copy().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33e1529d-6135-49b1-ab72-d8c0f960c912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'review_id', 'user_id', 'time', 'rating', 'text',\n",
      "       'pics_collapsed', 'resp_collapsed', 'name', 'description', 'category',\n",
      "       'url', 'image', 'is_image_ad', 'is_image_irrelevant', 'is_text_ad',\n",
      "       'is_text_irrelevant', 'is_text_rant', 'is_review_ad',\n",
      "       'is_review_irrelevant', 'helpfulness', 'sensibility'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(test_file.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f09808-377a-4a91-a471-439030e3239c",
   "metadata": {},
   "source": [
    "We evalute precision, recall, and F1-score for each class (True/False) and the overall weighted/macro scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dee140a7-754c-4d90-9fec-4de24c2b2026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== is_review_irrelevant ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      1.000     1.000     1.000        20\n",
      "\n",
      "    accuracy                          1.000        20\n",
      "   macro avg      1.000     1.000     1.000        20\n",
      "weighted avg      1.000     1.000     1.000        20\n",
      "\n",
      "=== is_review_ad ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      1.000     1.000     1.000        20\n",
      "\n",
      "    accuracy                          1.000        20\n",
      "   macro avg      1.000     1.000     1.000        20\n",
      "weighted avg      1.000     1.000     1.000        20\n",
      "\n",
      "=== is_text_rant ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False      1.000     1.000     1.000        20\n",
      "\n",
      "    accuracy                          1.000        20\n",
      "   macro avg      1.000     1.000     1.000        20\n",
      "weighted avg      1.000     1.000     1.000        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = test_file.merge(ground_truth, on=\"review_id\", suffixes=(\"_pred\", \"_true\"))\n",
    "targets = [\"is_review_irrelevant\", \"is_review_ad\", \"is_text_rant\"]\n",
    "\n",
    "for col in targets:\n",
    "    y_true = df[f\"{col}_true\"]\n",
    "    y_pred = df[f\"{col}_pred\"]\n",
    "    print(f\"=== {col} ===\")\n",
    "    print(classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffca02d-f101-4baf-a3e6-6dfc3eba9278",
   "metadata": {},
   "source": [
    "We subsequently iterated the process with alternative prompts. The prompts listed below were tested but yielded inferior performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
