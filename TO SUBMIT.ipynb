{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4589a9e3",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2fd4d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "777713bb-b282-468d-970e-04942240967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import gzip\n",
    "import json\n",
    "import io\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8347f1a",
   "metadata": {},
   "source": [
    "## Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f58bb5a",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae010a1",
   "metadata": {},
   "source": [
    "First, we import in the data for google reviews in Vermont, USA, and convert it into a dataframe entitled `vt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d2aa71c-65b6-482b-b6a8-3e38afb8e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://mcauleylab.ucsd.edu/public_datasets/gdrive/googlelocal/review-Vermont_10.json.gz'\n",
    "\n",
    "response = requests.get(url, stream = True)\n",
    "response.raise_for_status() \n",
    "\n",
    "with gzip.GzipFile(fileobj = io.BytesIO(response.content), mode = 'rb') as gz_file:\n",
    "    data_list = [json.loads(line) for line in gz_file]\n",
    "\n",
    "vt = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa7cd6fc-8da6-41de-98e2-3ab8303f030f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(324725, 8)\n",
      "Index(['user_id', 'name', 'time', 'rating', 'text', 'pics', 'resp', 'gmap_id'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(vt.shape) # 324725 observations and 8 columns.\n",
    "print(vt.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d922bb77-72e2-466a-a7cc-a4d554e69c18",
   "metadata": {},
   "source": [
    "Next, we import in the data for local businesses metadata in Vermont, USA, and convert it into a dataframe entitled `vt_metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e151a00-4ca4-4bde-9b95-8a7c6f5b3ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_metadata = 'https://mcauleylab.ucsd.edu/public_datasets/gdrive/googlelocal/meta-Vermont.json.gz'\n",
    "\n",
    "response_metadata = requests.get(url_metadata, stream = True)\n",
    "response_metadata.raise_for_status() \n",
    "\n",
    "with gzip.GzipFile(fileobj = io.BytesIO(response_metadata.content), mode = 'rb') as gz_file:\n",
    "    data_list1 = [json.loads(line) for line in gz_file]\n",
    "\n",
    "vt_metadata = pd.DataFrame(data_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1824f28-8fd0-496a-8c2d-6bc1e8d31b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11291, 15)\n",
      "Index(['name', 'address', 'gmap_id', 'description', 'latitude', 'longitude',\n",
      "       'category', 'avg_rating', 'num_of_reviews', 'price', 'hours', 'MISC',\n",
      "       'state', 'relative_results', 'url'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(vt_metadata.shape) # 11291 observations and 15 columns\n",
    "print(vt_metadata.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71683f6e-2bce-4708-961c-fa73675ec170",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Next, we clean and prepare  the two datasets for analysis. We start with the reviews dataset before moving on to the business metadata dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb993a",
   "metadata": {},
   "source": [
    "#### Reviews dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed24ef9-0d84-4199-8b77-6340413ee9bf",
   "metadata": {},
   "source": [
    "We'll perform the following steps on the reviews dataset:\n",
    "\n",
    "1. Drop the irrelevant `name` column to avoid redundancy as users can be identified by `user_id`\n",
    "2. Convert text in all columns to lower case\n",
    "3. Clean `text` by replacing multiple spaces with a single space\n",
    "4. Remove duplicate reviews based on user id, review text comment, business location, and time uploaded, to ensure each review is unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "326e17de-8126-4d56-aea5-b76dddc552b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vt = vt.drop('name', axis = 1) # drop 'name' column as it's not relevant to our analysis\n",
    "vt.columns = vt.columns.str.lower() # convert all strings to lower case\n",
    "vt['text'] = vt['text'].str.replace(r'\\s+', ' ', regex = True) # clean text \n",
    "vt = vt.drop_duplicates(subset = ['user_id', 'text', 'gmap_id', 'time']) # drop duplicate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf25292",
   "metadata": {},
   "source": [
    "Next, as the urls for the pictures in the `pics` column is presented in a nested dictionary, we write a function to collapse all urls into a single python list. We name the new column as `pics_collapsed` and drop the old `pics` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87d0f1eb-e7a9-4094-a959-2319b9e845c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_pics(pic_list):\n",
    "    if not pic_list:\n",
    "        return []  \n",
    "    urls = []\n",
    "    for pic_dict in pic_list:\n",
    "        urls.extend(pic_dict.get('url', []))\n",
    "    return urls\n",
    "\n",
    "vt['pics_collapsed'] = vt['pics'].apply(collapse_pics)\n",
    "vt = vt.drop('pics', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a068541e",
   "metadata": {},
   "source": [
    "Similarly, we write a function to collapse business responses to reviews in the `resp` column, naming the new column as `resp_collapsed` and dropping the old `resp` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38e6667d-398f-4016-a926-987e18d079d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texts(resp_entry):\n",
    "    if isinstance(resp_entry, dict):\n",
    "        # single response dict\n",
    "        return resp_entry.get(\"text\", \"\")\n",
    "    elif isinstance(resp_entry, list):\n",
    "        # list of response dicts\n",
    "        return \" \".join([d.get(\"text\", \"\") for d in resp_entry if isinstance(d, dict)])\n",
    "    elif isinstance(resp_entry, str):\n",
    "        # fallback: extract with regex if it's a string\n",
    "        texts = re.findall(r'\"text\":\\s*\"([^\"]*)\"', resp_entry)\n",
    "        return \" \".join(texts)\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "vt[\"resp_collapsed\"] = vt[\"resp\"].apply(extract_texts)\n",
    "vt = vt.drop(\"resp\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c1f6d",
   "metadata": {},
   "source": [
    "The first 3 observations of the reviews dataframe are as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8dbb1ee4-c426-46e1-8c2f-9a52ded07cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 user_id           time  rating  \\\n",
      "0  118026874392842649478  1620085852324       5   \n",
      "1  101532740754036204131  1580309946474       5   \n",
      "2  115404122636203550540  1605195974445       5   \n",
      "\n",
      "                                                text  \\\n",
      "0      Always done right from wood stove to screens!   \n",
      "1  A great company to work with. Their sales and ...   \n",
      "2  Great place to do business with staff was grea...   \n",
      "\n",
      "                                 gmap_id pics_collapsed  \\\n",
      "0  0x89e02445cb9db457:0x37f42bff4edf7a43             []   \n",
      "1  0x89e02445cb9db457:0x37f42bff4edf7a43             []   \n",
      "2  0x89e02445cb9db457:0x37f42bff4edf7a43             []   \n",
      "\n",
      "                                      resp_collapsed  \n",
      "0  Good Evening, Rebecca! Thanks SO much for the ...  \n",
      "1  Good Afternoon, Peter - Really appreciate the ...  \n",
      "2  Hi Chad!\\n\\nThank you so much for the 5-Star r...  \n"
     ]
    }
   ],
   "source": [
    "print(vt.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2659188a",
   "metadata": {},
   "source": [
    "#### Metadata dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce18aa",
   "metadata": {},
   "source": [
    "We'll perform the following steps on the reviews dataset:\n",
    "\n",
    "1. Drop the irrelevant columns that will not be needed for analysis\n",
    "2. Convert text in all columns to lower case\n",
    "3. Clean `description` and `category` columns by replacing multiple spaces with a single space\n",
    "4. Remove duplicate reviews based on business name and Google Maps id to ensure each observation is unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93279217-294d-4ffe-8670-a37f6576a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_metadata = vt_metadata.drop(['address', 'latitude', 'longitude', 'avg_rating', 'num_of_reviews', 'price', 'hours', 'MISC', 'state', 'relative_results'], axis = 1)\n",
    "vt_metadata.columns = vt_metadata.columns.str.lower() \n",
    "vt_metadata['description'] = vt_metadata['description'].str.replace(r'\\s+', ' ', regex = True)\n",
    "vt_metadata['category'] = vt_metadata['category'].str.replace(r'\\s+', ' ', regex = True)\n",
    "vt_metadata = vt_metadata.drop_duplicates(subset = ['name','gmap_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d04eb7",
   "metadata": {},
   "source": [
    "The first 3 observations of the metadata dataframe are as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a6df133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name                                gmap_id  \\\n",
      "0               Royal Group  0x89e02445cb9db457:0x37f42bff4edf7a43   \n",
      "1  Foxglove Farm and Forest  0x4cb549e8877cf0d7:0xe8f003e6d73392ae   \n",
      "2              Carr's Gifts  0x4cb54a301f3518f7:0x39af4eda1efb9117   \n",
      "\n",
      "  description  category                                                url  \n",
      "0        None       NaN  https://www.google.com/maps/place//data=!4m2!3...  \n",
      "1        None       NaN  https://www.google.com/maps/place//data=!4m2!3...  \n",
      "2        None       NaN  https://www.google.com/maps/place//data=!4m2!3...  \n"
     ]
    }
   ],
   "source": [
    "print(vt_metadata.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "467fb5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_metadata.to_csv('vt_metadata.csv')\n",
    "vt.to_csv('vt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c21119",
   "metadata": {},
   "source": [
    "### Data Scraping for Business Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b162deef",
   "metadata": {},
   "source": [
    "Scraping Process: We used the code below to derive a list of URLs that can be used by [Google Maps Scraper](https://console.apify.com/actors/nwua9Gu5YrADL7ZDj/input) to scrape data from Google Maps. Among the data collected are the `description` and `category` data. However, it has soon become too expensive to scrape all 11k+ entries using this scraper, so we instead used [Google Maps Review Scraper](https://console.apify.com/actors/nwua9Gu5YrADL7ZDj/input) to scrape `category` data, as well as the image URLs of the locations that can be used to retrieve the image to be used by Genma to generate a description for the place, as a potential substitute to textual descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f0fc9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting list of URLS \n",
    "import pandas as pd\n",
    "\n",
    "input_file = \"vt_metadata.csv\"\n",
    "output_file = \"urls.txt\"\n",
    "df = pd.read_csv(input_file)\n",
    "urls = df['url'].dropna().tolist()\n",
    "unique_urls = list(set(urls))\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    for url in unique_urls:\n",
    "        f.write(f\"{url}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd2023",
   "metadata": {},
   "source": [
    "The first 3 URLs in the file are as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71d953c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/maps/place//data=!4m2!3m1!1s0x4cb4ca87cefddb8b:0xcd5c11ad6acdc3aa?authuser=-1&hl=en&gl=us\n",
      "\n",
      "https://www.google.com/maps/place//data=!4m2!3m1!1s0x4cb507655eb10f69:0x85eacd66a9b02ac1?authuser=-1&hl=en&gl=us\n",
      "\n",
      "https://www.google.com/maps/place//data=!4m2!3m1!1s0x89e1caeb931f0ef1:0x987ee54770cc0466?authuser=-1&hl=en&gl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = open(\"urls.txt\", \"r\")\n",
    "for i in range(3):\n",
    "    print(file.readline())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f759da7",
   "metadata": {},
   "source": [
    "Numerous columns are available after the scraping. However, as only the `category` and `imageUrl` columns are of significance, aside from the URL that uniquely identifies each location, these are the only columns downloaded in the resulting dataset.\n",
    "\n",
    "The first 3 entries are as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c13a361c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  \\\n",
      "0  https://www.google.com/maps/place//data=!4m2!3...   \n",
      "1  https://www.google.com/maps/place/Orwell,+VT+0...   \n",
      "2  https://www.google.com/maps/place/Morristown,+...   \n",
      "\n",
      "                                               image  \\\n",
      "0  https://lh3.googleusercontent.com/gps-cs-s/AC9...   \n",
      "1  https://lh3.googleusercontent.com/gps-cs-s/AC9...   \n",
      "2  https://lh3.googleusercontent.com/gps-cs-s/AC9...   \n",
      "\n",
      "                                            category  \n",
      "0  ['Historical society', 'Gift shop', 'History m...  \n",
      "1                                          ['River']  \n",
      "2                                  ['Mountain pass']  \n"
     ]
    }
   ],
   "source": [
    "vt_scraped = pd.read_csv(\"imcat.csv\")\n",
    "print(vt_scraped.head(3)) # 11291 observations and 3 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964b9693",
   "metadata": {},
   "source": [
    "### Adding Scraped Data into Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e688c",
   "metadata": {},
   "source": [
    "We add the scraped `category` and `image` data into the metadata dataframe if these fields are empty in the metadata dataframe with the same `url`. Since the metadata dataframe does not have an `image` column, we created it before adding the data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2278f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Temp\\ipykernel_24756\\5542819.py:19: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['Historical society', 'Gift shop', 'History museum', 'Local history museum', 'Museum', 'Tourist attraction']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  vt_metadata.loc[matches & (vt_metadata['category'].isna() | (str(vt_metadata['category']).strip() == \"\")),\n",
      "C:\\Users\\angel\\AppData\\Local\\Temp\\ipykernel_24756\\5542819.py:24: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'https://lh3.googleusercontent.com/gps-cs-s/AC9h4no6TiFPpCZ6GEJ1Zjq8dmAx6qsjSsZ0UzqOiAfTXC5vNloE62FzpW7rHK5b--M1s2dplp_E2Klvc8ECkTELP-n_UJ6QUmnwDKEEUxibSyTsKOTOR7bb4e8fHeru4cb-uyzB2C-nK7k-4Clr=w408-h506-k-no' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  vt_metadata.loc[matches & (vt_metadata['image'].isna() | (str(vt_metadata['image']).strip() == \"\")),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file saved as urls.txt\n"
     ]
    }
   ],
   "source": [
    "# Add image column to vt_metadata if not exists\n",
    "if 'image' not in vt_metadata.columns:\n",
    "    vt_metadata['image'] = np.nan\n",
    "\n",
    "# Ensure consistent column names (lowercase, strip spaces)\n",
    "vt_metadata.columns = [col.strip().lower() for col in vt_metadata.columns]\n",
    "vt_scraped.columns = [col.strip().lower() for col in vt_scraped.columns]\n",
    " \n",
    "# We assume both have at least these columns: url, description, categories\n",
    "for idx, row in vt_scraped.iterrows():\n",
    "    url = row['url']\n",
    "    \n",
    "    # Find matching rows in main file\n",
    "    matches = vt_metadata['url'] == url\n",
    "    \n",
    "    if matches.any():\n",
    "        # Update categories if main file missing\n",
    "        if pd.notna(row['category']) and row['category'].strip() != \"\":\n",
    "            vt_metadata.loc[matches & (vt_metadata['category'].isna() | (str(vt_metadata['category']).strip() == \"\")),\n",
    "                        'category'] = row['category']\n",
    "        \n",
    "        # Update image if main file missing\n",
    "        if 'image' in row and pd.notna(row['image']) and row['image'].strip() != \"\":\n",
    "            vt_metadata.loc[matches & (vt_metadata['image'].isna() | (str(vt_metadata['image']).strip() == \"\")),\n",
    "                        'image'] = row['image']\n",
    "        \n",
    "\n",
    "# Save the updated file\n",
    "vt_metadata.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Updated file saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8afe86",
   "metadata": {},
   "source": [
    "### Merging Reviews and Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484fd1d",
   "metadata": {},
   "source": [
    "We merge the reviews and metadata dataframes by common key `gmap_id`, save the merged dataframe as `vt_merged`. We drop the now irrevelent `gmap_id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9596d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_merged = pd.merge(vt, vt_metadata, on = 'gmap_id', how = 'inner')\n",
    "vt_merged = vt_merged.drop('gmap_id', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6790182e",
   "metadata": {},
   "source": [
    "We insert new column called `review_id` such that each review has a unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d2cbe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_merged['review_id'] = range(len(vt_merged)) \n",
    "id_column = vt_merged.pop('review_id')\n",
    "vt_merged.insert(0, 'review_id', id_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f24c4d7",
   "metadata": {},
   "source": [
    "### Manual Labelling of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15527c7",
   "metadata": {},
   "source": [
    "We add 7 new boolean columns:\n",
    "1. `is_image_ad` TRUE if image uploaded is labelled as advertisement\n",
    "2. `is_image_irrelevant` TRUE if image uploaded is labelled as irrelevant\n",
    "3. `is_text_ad` TRUE if text comment is labelled as advertisement\n",
    "4. `is_text_irrelevant` TRUE if text comment is labelled as irrelevant\n",
    "5. `is_text_rant` TRUE if text comment is labelled as rant from non-visitor\n",
    "6. `is_review_ad` TRUE if either image or text comment is labelled as advertisement \n",
    "7. `is_review_irrelevant` TRUE if either image or text comment is labelled as irrelevant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "48b710f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_merged['is_image_ad'] = None\n",
    "vt_merged['is_image_ad'] = vt_merged['is_image_ad'].astype(bool)\n",
    "\n",
    "vt_merged['is_image_irrelevant'] = None\n",
    "vt_merged['is_image_irrelevant'] = vt_merged['is_image_irrelevant'].astype(bool)\n",
    "\n",
    "vt_merged['is_text_ad'] = None\n",
    "vt_merged['is_text_ad'] = vt_merged['is_text_ad'].astype(bool)\n",
    "\n",
    "vt_merged['is_text_irrelevant'] = None\n",
    "vt_merged['is_text_irrelevant'] = vt_merged['is_text_irrelevant'].astype(bool)\n",
    "\n",
    "vt_merged['is_text_rant'] = None\n",
    "vt_merged['is_text_rant'] = vt_merged['is_text_rant'].astype(bool)\n",
    "\n",
    "vt_merged[\"is_review_ad\"] = vt_merged[\"is_text_ad\"] | vt_merged[\"is_image_ad\"]\n",
    "\n",
    "vt_merged['is_review_irrelevant'] = vt_merged[\"is_image_irrelevant\"] | vt_merged[\"is_text_irrelevant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe352c7",
   "metadata": {},
   "source": [
    "Next, we add 2 columns for quality check of the review.\n",
    "1. `helpfulness`\n",
    "    - `not_helpful` (review was relevant but did not add information to the reader)\n",
    "    - `helpful` (review provided some helpful information to make decisions about the visit)\n",
    "    - `very_helpful` (review gave crucial or new information that can significantly impact visit decisions)\n",
    "\n",
    "2. `sensibility` TRUE if the numerical star rating corresponds to the sentiments present in text review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de8f1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"not_helpful\", \"helpful\", \"very_helpful\"]\n",
    "vt_merged[\"helpfulness\"] = pd.Categorical(\n",
    "    values=[\"\"] * len(vt_merged), \n",
    "    categories = categories,\n",
    "    ordered = True\n",
    ")\n",
    "\n",
    "vt_merged['sensibility'] = None\n",
    "vt_merged['sensibility'] = vt_merged['sensibility'].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5316029f",
   "metadata": {},
   "source": [
    "The columns of the dataframe are as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1353e355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['review_id', 'user_id', 'time', 'rating', 'text', 'pics_collapsed',\n",
      "       'resp_collapsed', 'name', 'description', 'category', 'url', 'image',\n",
      "       'is_image_ad', 'is_image_irrelevant', 'is_text_ad',\n",
      "       'is_text_irrelevant', 'is_text_rant', 'is_review_ad',\n",
      "       'is_review_irrelevant', 'helpfulness', 'sensibility'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(vt_merged.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c03bbf3",
   "metadata": {},
   "source": [
    "### Filtering for Reviews Containing Text and/or Pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa9bd6",
   "metadata": {},
   "source": [
    "We assume that a numerical star rating alone does not provide sufficient ustification on why a review would violate any of the three policies. Hence, in this project we only focus on looking at the reviews containing text comments and/or pictures. This filters out around 45% of all reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5e127401",
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_rating_only = vt_merged[\n",
    "    vt_merged[\"text\"].isna() & (vt_merged[\"pics_collapsed\"] == \"[]\")\n",
    "]\n",
    "\n",
    "vt_merged = vt_merged.drop(vt_rating_only.index) # contains text and/or pic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a080df",
   "metadata": {},
   "source": [
    "We save the final dataframe as `vt_merged.csv`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
